% THIS IS SIGPROC-SP.TEX - VERSION 3.1
% WORKS WITH V3.2SP OF ACM_PROC_ARTICLE-SP.CLS
% APRIL 2009
%
% It is an example file showing how to use the 'acm_proc_article-sp.cls' V3.2SP
% LaTeX2e document class file for Conference Proceedings submissions.
% ----------------------------------------------------------------------------------------------------------------
% This .tex file (and associated .cls V3.2SP) *DOES NOT* produce:
%       1) The Permission Statement
%       2) The Conference (location) Info information
%       3) The Copyright Line with ACM data
%       4) Page numbering
% ---------------------------------------------------------------------------------------------------------------
% It is an example which *does* use the .bib file (from which the .bbl file
% is produced).
% REMEMBER HOWEVER: After having produced the .bbl file,
% and prior to final submission,
% you need to 'insert'  your .bbl file into your source .tex file so as to provide
% ONE 'self-contained' source file.
%
% Questions regarding SIGS should be sent to
% Adrienne Griscti ---> griscti@acm.org
%
% Questions/suggestions regarding the guidelines, .tex and .cls files, etc. to
% Gerald Murray ---> murray@hq.acm.org
%
% For tracking purposes - this is V3.1SP - APRIL 2009

\documentclass{acm_proc_article-sp}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage[noend]{algpseudocode}
\algrenewcomment[1]{\(\triangleright\) #1}
\usepackage{hyperref}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}


\begin{document}

\title{Data Fusion Using Source Trustworthiness}
%
% You need the command \numberofauthors to handle the 'placement
% and alignment' of the authors beneath the title.
%
% For aesthetic reasons, we recommend 'three authors at a time'
% i.e. three 'name/affiliation blocks' be placed beneath the title.
%
% NOTE: You are NOT restricted in how many 'rows' of
% "name/affiliations" may appear. We just ask that you restrict
% the number of 'columns' to three.
%
% Because of the available 'opening page real-estate'
% we ask you to refrain from putting more than six authors
% (two rows with three columns) beneath the article title.
% More than six makes the first-page appear very cluttered indeed.
%
% Use the \alignauthor commands to handle the names
% and affiliations for an 'aesthetic maximum' of six authors.
% Add names, affiliations, addresses for
% the seventh etc. author(s) as the argument for the
% \additionalauthors command.
% These 'additional authors' will be output/set for you
% without further effort on your part as the last section in
% the body of your article BEFORE References or any Appendices.

\numberofauthors{3} %  in this sample file, there are a *total*
% of EIGHT authors. SIX appear on the 'first-page' (for formatting
% reasons) and the remaining two appear in the \additionalauthors section.
%
\author{
% You can go ahead and credit any number of authors here,
% e.g. one 'row of three' or two rows (consisting of one row of three
% and a second row of one, two or three).
%
% The command \alignauthor (no curly braces needed) should
% precede each author name, affiliation/snail-mail address and
% e-mail address. Additionally, tag each line of
% affiliation/address with \affaddr, and tag the
% e-mail address with \email.
%
% 1st. author
\alignauthor
Manual Heinkel \\
       \affaddr{UMass Amherst}\\
       \affaddr{140 Governors Drive}\\
       \affaddr{Amherst, Massachusetts}\\
       \email{heinkel@cs.umass.edu}
% 2nd. author
\alignauthor
Nicholas Monath \\
       \affaddr{UMass Amherst}\\
       \affaddr{140 Governors Drive}\\
       \affaddr{Amherst, Massachusetts}\\
       \email{nmonath@cs.umass.edu}
% 3rd. author
\alignauthor 
Lakshmi Nair \\
       \affaddr{UMass Amherst}\\
       \affaddr{140 Governors Drive}\\
       \affaddr{Amherst, Massachusetts}\\
       \email{lvnair@cs.umass.edu}
}
\date{8 March 2015}
% Just remember to make sure that the TOTAL number of authors
% is the number that will appear on the first page PLUS the
% number that will appear in the \additionalauthors section.

\maketitle


% A category with the (minimum) three required fields
%\category{H.4}{Information Systems Applications}{Miscellaneous}
%A category including the fourth, optional field follows...
%\category{D.2.8}{Software Engineering}{Metrics}[complexity measures, performance measures]

%\terms{Theory}

%\keywords{ACM proceedings, \LaTeX, text tagging} % NOT required for Proceedings

\section{Introduction}

The advent of data sources on the web in the last two decades has sparked the need for automated methods of combining sources into a single source. As these data sources often contain noisy and incorrect data, techniques for discerning the correct value among a set of conflicting values must be developed. These techniques solve the problem of data fusion. In data fusion, we consider information or $attributes$ of $entities$ provided by various $sources$. The entities include companies, airplane flights, books etc and attributes for these entities include open and close prices, time delays and isbn numbers respectively. Data sources include any site or database on the web. Data fusion is challenging in that the data may be stored in a variety of formats. Values for an entity's attributes may be semantically equivalent without having the same representation (e.g \$104Mil and 104,000,000 as a company's holding or Ernest Hemingway and E. Hemingway as the author of a book). In certain cases, the range of values for an attribute is not known. The decision of selecting the correct value for an attribute must be made under a great deal of uncertainty in a situation that is heavily domain dependent. Typically, no prior knowledge is known about the authenticity of the data sources. Labeled data is scarce and so supervised methods for data fusion are not practical. Most approaches to data fusion are unsupervised and make use of algorithms such as Expectation-Maximization (EM). A further difficulty is that the causes of erroneous data cannot easily be modeled by common distributions. Many of the current approaches to data fusion in the literature currently focus on modeling the trustworthiness of a data source. However, it is possible that a data source reliably provides values for one attribute, but not others. For example, Orbitz might accurately provide the departure time of a flight while frequently providing an incorrect value for the delay of a flight. In this work, we present models of the trustworthiness each attribute provided by a source. Attributes of entities are often assumed to be independent. We propose a possible model which captures the trustworthiness of sources as well as dependence among attributes.  Other methods in the literature model copying between data sources and the variance of the attributes. Few approaches consider all of these well studied features. We hope to investigate if we can come up with a model which covers all of these features. 

\section{Related Work}
Data fusion is also referred to as truth discovery \cite{waguih:truth} \cite{yin:truth} and data integration \cite{sarma:data} \cite{zhao:bayesian}. While there has been some work on how data fusion pertains to information extraction, specifically question answering \cite{wu:corroborating}, the significant body of literature pertains to the construction of databases, particularly relational databases and, recently, knowledge bases \cite{dong:data}. 



The problem of data fusion in relational databases typically consists of multiple phases \cite{bleiholder:data} \cite{li:truth}. Given a collection of data sources, which all contain data that is to be stored in a particular relation, the first step is often the mapping of the data sources' schema to the schema of the underlying relation \cite{bleiholder:data}. Several techniques for schema mapping are discussed in \cite{naumann:data}. Several papers such as \cite{li:truth} address the process of selecting the true data values from the sources independently of schema mapping as we do in this work.


A naive approach to this problem is to resolve the conflicts by performing a majority vote among the sources. Early approaches such as {\sc TruthFinder} \cite{yin:truth} extend this technique by incorporating a notion of source trustworthiness, i.e. how much one should trust the value presented by a source. {\sc TruthFinder} follows a heuristic which assumes that a source which provides mostly true claims for many data items is likely to provide true claims for other data items. The estimation of the trustworthiness of data sources   is a common thread among data fusion algorithms as noted by \cite{li:truth} \cite{waguih:truth}. Other truth discovery algorithms which are based on corroboration have been proposed by Galland et al. \cite{galland:corro}. Three algorithms were proposed ({\sc Cosine}, {\sc 2-Estimates} and {\sc 3-Estimates}) which estimate the truth in the values and the trust in the views. {\sc Cosine} is a heuristic approach which is based on the cosine similarity in information retrieval. The algorithm is initialized with the confidence of each value and the source trustworthiness. The algorithm then iteratively recomputes the source trustworthiness as a linear function of the previous iteration. 


Other approaches such as Latent Truth Model proposed by Zhao et al. \cite{zhao:bayesian} use probabilistic Bayesian models. In contrast to previous methods, this model captures the distinction between false positive and false negative errors injected by the source. LTM models the probability of each fact being true as a latent random variable and the actual truth label of each fact as a latent boolean random variable. A Collapsed Gibbs Sampling algorithm is used for inferring the true labels. The algorithm works by randomly initializing the truth labels for each fact and in each iteration the truth values are sampled from a distribution conditioned on all current truth labels. 

Another method proposed by Wang et al. \cite{wang2012truth} is Maximum Likelihood Estimation. Unlike most of the other methods, which are heuristic, this approach offers an optimal solution to the problem of truth discovery. The model parameters include the probability of a value being reported as true by the source and the probability that value is true given the source. The Expectation Maximization Algorithm is used to find optimal values for these parameters. During the Expectation step, the probability that value is true given source probabilities is computed and during the Maximization step, the source probabilities are computed. This method has been shown to work well in cases where there is a lot of noise in the input data. 

Related algorithms such as Latent Credibility Analysis (LCA) \cite{pasternack:latent} model the truthfulness of facts presented by sources using a probabilistic graphical model. LCA represents the truth of a given fact as a latent variable in the graphical model. LCA jointly models the truth of a given fact, the trustworthiness of all of the data sources and other factors such as the variability of particular attributes. Unlike many other works, LCA can be trained not only in an unsupervised way, but also in a semi-supervised way. LCA follows up earlier work done by the same authors such as \cite{pasternack:knowing} and \cite{pasternack:making}, which uses an interesting trick of placing sources into several groups in the truth finding process. 


Data sources may also contain noise as a result of copying values from one another. A novel contribution of \cite{dong:integrating} is a model of the dependence between sources. The {\sc AccuCopy} method presented in \cite{dong:integrating} and in the survey \cite{li:truth} models the dependence by estimating the probability a source would independently come up with its claim using a Bayesian approach. 

The problem of data fusion is extended to data streams in the recent work \cite{zhao:truth}. The probabilistic approach performs fusion via source quality estimation in real time. While the approach could be used on fixed data as well, the efficiency of this approach is an important contribution. 

The recent work of Li et al. \cite{li:resolving} has been shown to be state of the art on several standard datasets such as the stock dataset \cite{li:truth}, the flight dataset \cite{li:truth} and others. The work frames data fusion as a regularized optimization problem such that the loss function cleverly combines a representation of source trustworthiness and the values proposed by each source while respecting the domain of the attributes. As in LCA and other methods, the trustworthiness is calculated jointly. As one of the top performing systems, this work will be a point of comparison and extension in our project. 

Recent work has focused on knowledge fusion, a closely related problem to data fusion \cite{dong:data} \cite{pochampally:fusing} \cite{yu:wisdom}.  In this task the data are knowledge base entries such as a pair of entities and a relation rather than a collection of attributes. Many data fusion techniques can be effectively applied to this problem as shown in \cite{dong:data}. Handling data sources which contain correlations is addressed by \cite{pochampally:fusing}. Correlations between sources include not only copying, but also factors such as similarities between the extraction techniques used. Modeling correlations rather than copied values is a novel contribution by  \cite{pochampally:fusing}.


Another related research area is the collection of labels from crowdsourcing \cite{nguyen:minimizing}. Particularly, techniques for using crowdsourcing to create labeled data sets \cite{sheng:get} \cite{nguyen:minimizing} and crowdsourcing for multiple choice question answering \cite{bachrach:grade}. In these approaches the number of sources/users can be much greater than for usual data fusion problems \cite{li:truth} \cite{nguyen:minimizing}.

We expect our work to differ from the previous work in that we will model source trustworthiness not only globally, i.e. for all attributes, but also locally for each attribute. In doing so, we hope to capture a more fine grained representation of source trustworthiness. We will also attempt to ensemble existing algorithms using a variety of techniques such as weighted ensemble. We hope to use semi-supervised techniques in situations where more domain-knowledge could be useful. Most of the models presented previous works such as \cite{pasternack:latent} and \cite{li:resolving} do not attempt to measure the dependence of attributes on other attributes. We hope to investigate if there is a trac
 way to capture these dependencies and if we can use them to improve data fusion methods.

\section{Models}

\subsection{Problem Description}

We provide the following formalism for the problem of data fusion. A $record$, $r$, is provided by a $source$, $s$, about an $entity$, $e$. A $source$ is a supplier of data. For example, Expedia is a source for flight data, and Yahoo Finance is a source of stock data.  An $entity$ is a real world object about which a $source$ provides data. The entities about which Expedia provides data are flights and Yahoo Finance provides data about a company's stock price. An entity has a unique identifier. In the case of flight data this would be the flight number. In the case of stock data, this would be the company's ticker symbol. A $record$ contains one or more $attributes$, $a_1,\ a_2,\dots,\ a_n$, of an entity. Each $attribute$ has a specific domain. In the case of flights, the attribute $time$ $delay$ might be an integer and for stocks, the $opening$ $price$ might have a domain of positive real numbers.  The domain of an attribute may also be binary (true/false) or categorical. We refer to the collection of records provided by a source $s$ as $R_s$. The $schema$ of a record is the set of attributes which each entity is expected to contain.

Given a set of sources $S$ and a collection of records $R_s$ for each $s \in S$, the problem of \emph{data fusio}n is to combine all collections $R_s$ into a single collection $\hat{R}$ which estimates the true assignment of values to the attributes for entities.  Each of the following algorithms provides a different technique for converting a set of $R_s$ collections into $\hat{R}$. 

\vspace {1cm}

\subsection{Data Sets}
To evaluate our algorithms we consider the \emph{stock data set}\footnote{\url{http://lunadong.com/fusionDataSets.html}} and the \emph{flight data set} which were introduced in \cite{li:truth}. Some statistics of the data set is shown in table \ref{fig:stock}. This stock data set contains stock information obtained in July 2011 by using 55 different sources like \emph{Google Finance, Bloomberg} or \emph{NASDAQ}. From each of these sources 1000 stocks were considered. In July 2011 on each weekday the stock data of each stock was collected from each of the 55 sources. Different sources had different numbers of attributes. The \emph{clean stock data set} which we use contains 16 attributes per stock which had the best coverage across the sources. Among these 16 attributes are: \emph{change in \%, opening price, todays high/low, market capacity, dividend, etc}. For the \emph{stock data}, \emph{Xian Li et al.} \cite{li:truth} also generated a gold standard for evaluation.
The flight data set contains flight information which was collected during December 2011. Data was collected from 38 sources and 1200 flights were considered. The clean flight data set includes 6 attributes.


%todo: change vertical padding
\begin{figure}[h]
    \centering
{%\footnotesize
\begin{tabular}{  | c | c | c | c  | c | c |}		
\hline
\textbf{} & \textbf{Src} & \textbf{Period} & \textbf{Entitys} & \textbf{Attrs}  & \textbf{Items}  \\
\hline
 Stock & 55 & July 2011 &  1000 & 16 &  16000*55   \\
 Flight & 38 & Dec 2011 &  1200 & 6 &  72000*38   \\
\hline
\end{tabular}
}
 \caption{Stock data collection overview}%
    \label{fig:stock}%
\end{figure}

\subsection{Existing Methods}

\subsubsection{Majority Voting}

A simple approach to data fusion is collect all of the candidate values provided by the sources for a particular attribute of an entity and assign the attribute the value that appears most frequently. While general idea appears in all of the following   algorithms,  it is flawed in that it ignores information about the data sources, uncertainty of the attributes and the relationships between  attributes.


\subsubsection{2-Estimate}
The {\sc 2- Estimate} algorithm was proposed by Galland et al. in \cite{galland:corro}. This algorithm is related to a probabilistic model to estimate the source trustworthiness and the confidence of an attribute value. To compute the value confidence the algorithm takes the disagreeing sources into consideration. First, the confidence of a value is computed by analyzing agreeing and disagreeing sources of that value. After that the value confidence is normalized with a \emph{normalization function}. Then the algorithm recomputes the source trustworthiness by looking at the confidence of the different values provided by that source. After normalizing the source trustworthiness this process is repeated until convergence. Listing \ref{alg:2e} shows the algorithm in pseudocode.

In the following algorithm, $\mathsf{Provides}(s,\alpha)$ is an indicator function, which is 1 if a source provides the value $\alpha$ for attribute $a$ and 0 otherwise, and $\mathsf{Provides}(s,a)$ is an indicator function, which is 1 if a source provides any value for attribute $a$ and 0 otherwise. 

\begin{algorithm}[H]
	\small
\caption{2-Estimates}
\label{alg:2e} 
\begin{algorithmic}[1]
	\Function{2-Estimates}{$R$, $\delta$, $\lambda$, $t_0$} \\
	\Comment{Inputs: The collection of records $R$ and the hyper parameters.} \\
\Comment{Output: Returns the fused data $\hat{R}$} \\
	\For{\textbf{each} $s$ \textbf{in} $S$}
		\State  $\mathsf{TRUST}(s) \gets t_0$
	\EndFor

\While{\textbf{not} $Converged$($\mathsf{TRUST},\ \delta$)}
	\For{\textbf{each} $e \in E$}
		\For{\textbf{each} $a \in Attr(e)$}
			\State $pos \leftarrow \sum_{s\in S} \mathsf{Provides}(s,\alpha){(1- \mathsf{TRUST}(s))}$
			\State $neg \gets \sum_{s\in S} (1-\mathsf{Provides}(s,\alpha)){\mathsf{TRUST}(s)}$
			\State $\mathsf{CONF}(\alpha) \gets \frac{pos + neg}{\sum_{s \in S} \mathsf{Provides}(S,a)}$ 
		\EndFor
	\EndFor
	\State $\mathsf{CONF} \leftarrow {\sc Normalize}(\mathsf{CONF},\lambda)$

	\For{\textbf{each} $s \in S$}
		\State $pos \gets \sum_{a\in Attr(\cdot)}{\mathsf{Provides}(s,a)(1- \mathsf{CONF}(a))}$
		\State $neg \gets \sum_{a\in Attr(\cdot)}\mathsf{Provides}(s,a)\cdot{\mathsf{CONF}(a)}$ 
		\State $\mathsf{TRUST}(s) \gets \frac{pos + neg}{\sum_{a\in Attr(\cdot)}{\mathsf{Provides}(s,a)}}$ 
	\EndFor
	\State $\mathsf{TRUST} \gets Normalize(\mathsf{TRUST},\lambda)$

	\State Initialize $\hat{R}$ to an empty collection of records
	\For{\textbf{each}  $e$ \textbf{in} $E$}
		\For{\textbf{each} $a$ \textbf{in} $Attr(e)$}
		\State $\hat{\alpha} \leftarrow \argmax_{\alpha} \mathsf{CONF}(\alpha)$
		\EndFor
		\State Add record for $e$ with attrs \& vals $(a, \hat{\alpha})$ to $\hat{R}$.
	\EndFor

\EndWhile
\State \textbf{return} $\hat{R}$
\EndFunction

 \\

\Function{Normalize}{$A$, $\lambda$} \\
\Comment{Inputs: A vector $A$ and value $\lambda$} \\
\Comment{Outputs: A normalized version $A$.} \\

\State \textbf{return} $\lambda \cdot \left (\frac{A - \min(A)}{\max(A)-\min(A)} \right) + (1-\lambda) \cdot ${\sc Round}$( A )$

\EndFunction
\end{algorithmic}
\end{algorithm}  

%% Lakshmi & Manuel please feel free to change these variable names if you want to. I'm in no way attached to them, it was just an initial stab at this. 
\begin{table}
\centering
\begin{tabular}{|c|c|}
\hline
Variable & Description  \\
\hline
$s$ & Data source \\
$S$ & Set of all sources  \\
$e$ & Entity \\
$E$ & Set of all entities \\
$a_i$ & The $i^{th}$ Attribute of $e$ \\
$\alpha_i^{(s)}$ & The value of $a_i$ given by $s$ \\
$Attr(e)$ & The attributes of $e$ from training data\\
$Attr^g(e)$ & The attributes of $e$ from ground truth data\\
$Attr(\cdot)$ & The attributes of all of the entities, $\cup_{e \in E} Attr(e)$ \\
$r$ & Record (an entity and its attributes) \\
$R$ & A collection of records \\
$R_s$ & The records provided by a source $s$. \\
\hline
\end{tabular}
\caption{Definition of each variable used in the algorithms.}
\label{tbl:vars}
\end{table}

\begin{table}
\centering
\begin{tabular}{|c|c|}
\hline
Variable & Example \\
\hline
$s$  & MarketWatch \\
$S$ & \{MarketWatch,YahooFinance,$\dots$\}  \\
$e$ & GM (General Motors) \\
$E$ & \{GM, AAPL, F, $\dots$\} \\
$a_i$ & Opening Price \\
$\alpha_i^{(s)}$ & \$37.50 \\
$Attr(e)$ & \{Open Price, Avg Price, $\dots$\} \\
\hline
\end{tabular}
\caption{Examples for each of the variable types.}
\label{tbl:varExs}
\end{table}

\subsubsection{TruthFinder}

Yin et al present a Bayesian approach to data fusion in \cite{yin:truth}.  The likelihood of an entity's attribute having a particular value is a function of both the trustworthiness of the source providing the value as well as the values presented by other sources. The model is based on the idea that a source is trustworthy if it frequently provides a correct value and a value is often correct for an attribute if the providing source is trustworthy. However, as the algorithm is unsupervised we model the trustworthiness of a source as a function of the likelihood of correctness of the attributes it provides. 

Specifically, let $\alpha_i^{(s)}$ be the value provided by source $s$ for the attribute $a_i$ of entity $e$. The $trustworthiness$ of $s$ is denoted: $\mathsf{TRUST}(s)$. Based solely on the trustworthiness of the sources, the likelihood of correctness or confidence we have that $\alpha_i^{(s)}$ is the correct value for $a_i$ is denoted $\mathsf{CONF(\alpha_i^{(s)})}$.  The confidence is defined (in log space) as:

\begin{equation}
\mathsf{CONF(\alpha_i^{(s)})} = \sum_{s' \in S} \mathsf{Provides}(s',\alpha_i^{(s)}) \cdot \ln(1-\mathsf{TRUST}(s'))
\end{equation}

where $\mathsf{Provides}(s',\alpha_i^{(s)})$ is an indicator function that is 1 only if $s'$ provides $\alpha_i^{(s)}$ as the value for $a_i$ and 0 otherwise. $\mathsf{CONF(\alpha_i^{(s)})}$ is the log probability that each of the sources providing the value $\alpha_i^{(s)}$ are correct. This confidence value is then used to create an $adjusted$ confidence $\mathsf{CONF^\star(\alpha_i^{(s)})}$.

\begin{align}
\mathsf{CONF^\star(\alpha_i^{(s)})} &= \mathsf{CONF(\alpha_i^{(s)})} + \\ \nonumber
&  \sum_{s' \in S \setminus \{s\}} \rho \cdot \mathsf{CONF(\alpha_i^{(s')})}  \cdot \mathsf{SIM}(\alpha_i^{(s)},\alpha_i^{(s')})
\end{align}

where $\mathsf{SIM}$ is a similarity function defined  for domain the attribute $a_i$. The function $\mathsf{CONF}^\star$ can be view as increasing  the $\mathsf{CONF}$ of the value of an attribute if trustworthy sources provide similar values and decreasing it otherwise. As mentioned before the trustworthiness of a source based on the confidence values for the attributes it provides. 

\begin{equation}
\mathsf{TRUST}(s) = \frac{1}{Z} \sum_{e \in E}\sum_{a \in Attr(e)} \sigma(\gamma \cdot \mathsf{CONF}^\star(\alpha^{(s)}))
\end{equation}

where $Z$ is the number of values (across all entities) provided by $S$ and $\sigma$ is the sigmoid function, which is used to map $\mathsf{CONF}^\star$ to a value between 0 and 1.

\begin{equation}
\sigma(x) = \frac{1}{1 + \exp{(-x)}}
\end{equation}

The final value assigned to an entity's attribute is the value with the highest confidence. The algorithm is listed in Algorithm \ref{alg:tf} 

\begin{algorithm}
\caption{TruthFinder}
\begin{algorithmic}[1]
\small
\Function{TruthFinder}{$R$, $\delta$, $\rho$, $\gamma$, $t_0$} \\
\Comment{Inputs: The collection of records $R$ and the hyper parameters.} \\
\Comment{Output: Returns the fused data $\hat{R}$} \\

\For{\textbf{each} $s$ \textbf{in} $S$}
\State $\mathsf{TRUST}(s) \leftarrow t_0$
\EndFor
\\
\While{\textbf{not} Converged($\mathsf{TRUST}, \delta$)}
\For{\textbf{each}  $e$ \textbf{in} $E$}
\For{\textbf{each} $a$ \textbf{in} $Attr(e)$}
\State $\mathsf{CONF(\alpha^{(s)})} \leftarrow \sum_{s' \in S} \mathsf{Provides}(s',\alpha^{(s)}) \cdot \ln(1-\mathsf{TRUST}(s'))$
\EndFor
\For{\textbf{each} $a$ \textbf{in} $Attr(e)$}
\State $\mathsf{CONF^\star(\alpha^{(s)})} \leftarrow \mathsf{CONF(\alpha^{(s)})} +   \sum_{s' \in S \setminus \{s\}} \rho \cdot \mathsf{CONF(\alpha^{(s')})}  \cdot \mathsf{SIM}(\alpha^{(s)},\alpha^{(s')})$
\EndFor
\EndFor
\For{\textbf{each} $s$ \textbf{in} $S$}
\State $\mathsf{TRUST}(s) \leftarrow avg \big ( \sum_{e \in E}\sum_{a \in Attr(e)} \sigma(\gamma \cdot \mathsf{CONF}^\star(\alpha^{(s)})) \big )$
\EndFor
\EndWhile

\\
\State Initialize $\hat{R}$ to an empty collection of records

\For{\textbf{each}  $e$ \textbf{in} $E$}
\For{\textbf{each} $a$ \textbf{in} $Attr(e)$}
\State $\hat{\alpha} \leftarrow \argmax_{\alpha} \mathsf{CONF}(\alpha)$
\EndFor
\State Add record for $e$ with attrs \& values $(a, \hat{\alpha})$ to $\hat{R}$.
\EndFor
\\

\State \textbf{return} $\hat{R}$
\EndFunction
\end{algorithmic}
\label{alg:tf}
\end{algorithm}  





\subsubsection{Maximum Likelihood Estimation}
Wang et al. \cite{wang2012truth} proposed a  method to jointly estimate the maximum likelihood of both source trustworthiness and attribute value correctness. Expectation maximization (EM) algorithm is used for estimating these values.  This algorithm only deals with positive Boolean-valued attributes, however categorical attribute values can be cast into this framework. A pair of parameters is associated with each source. Let $t(s)$ denotes the probability that a source reports a value to be true when its value is actually true and $f(s)$ denotes the probability that a source reports a value to be true when its value is actually false.  Let ${t_a}$ and ${f_a}$ denote the conditional probability of attribute value $a$ being true given the source probabilities $t(s)$ and $f(s)$.  The algorithm works by alternately computing the attribute value probabilities based on source probabilities in the expectation step and then using these probabilities to update the source parameters in the maximization step.  The confidence values associated with each attribute are also computed in the expectation step. This is continued until convergence.  The pseudo code for this algorithm is shown in \ref{alg:ml} .

\clearpage
\begin{algorithm}
\caption{Maximum Likelihood Estimation}
\begin{algorithmic}[1]
\small
\Function{MLE}{$R$, $\beta$, $\delta$, $r$} \\
\Comment{Inputs: The collection of records $R$ and the hyper parameters.} \\
\Comment{Output: Returns the fused data $\hat{R}$} \\


\For{\textbf{each} $s$ \textbf{in} $S$}
\State $ k(s) \leftarrow \frac{|a_i^s|}{\sum_s|a_i^s|} $
\State $ t(s) \leftarrow \frac{r  \cdot k(s)}{\beta} $
\State $ f(s) \leftarrow \frac{(1-r) \cdot k(s)}{1-\beta} $
\EndFor

\\
\While{\textbf{not} Converged($\mathsf{a}, {b},\delta$)}
\State $\mathsf{CONF_{sum}} \leftarrow 0 $
\For{\textbf{each}  $e$ \textbf{in} $E$}
\For{\textbf{each} $a$ \textbf{in} $Attr(e)$}
\State $ t_a \leftarrow 1 $
\State $ f_a \leftarrow 1 $

\For{\textbf{each} $s$ \textbf{in} $S$}
 \If{$a$ \textbf{in} $s$ }  
\State $ t_a \leftarrow t_a  \cdot t(s) $
\State $ f_a \leftarrow f_a   \cdot f(s) $
\Else
\State $ t_a \leftarrow t_a  \cdot (1-t(s)) $
\State $ f_a \leftarrow f_a   \cdot (1-f(s)) $
\EndIf
\EndFor
\State $ \mathsf{CONF}(a) \leftarrow \frac{t_a \cdot \beta}{t_a \cdot \beta + f_a \cdot (1-\beta)} $
\State $\mathsf{CONF_{sum}} \leftarrow \mathsf{CONF_{sum}} + \mathsf{CONF}(a) $
\EndFor
\EndFor

\For{\textbf{each} $s$ \textbf{in} $S$}
\State $\mathsf{CONF_s}(sum) \leftarrow \sum_i CONF(a_i^s)  $
\State $ t(s) \leftarrow \frac{\mathsf{CONF}_s(sum)}{\mathsf{CONF}_{sum}}  $
\State $ f(s) \leftarrow  |a_i^s| - \frac{\mathsf{CONF_s}(sum)}{\sum_s|a_i^s| - \mathsf{CONF_{sum}}}  $
\EndFor


\EndWhile

\\

\For{\textbf{each}  $e$ \textbf{in} $E$}
\For{\textbf{each} $a$ \textbf{in} $Attr(e)$}
 \If{CONF(a) > 0.5}
  \State Add record for $e$ with attributes $a$ to $\hat{R}$.
\EndIf
\EndFor
\EndFor
\\

\State \textbf{return} $\hat{R}$
\EndFunction
\end{algorithmic}
\label{alg:ml}
\end{algorithm}  






\subsection{Conflict Resolution on Heterogenous Data}

Li et al present an extension of the {\sc TruthFinder} algorithm in the recent paper \cite{li:resolving} which achieves state of the art results. The algorithm poses data fusion as the following optimization problem: 

\begin{align}
\hat{R}, W^* = \argmin_{R,W} f(R, W) & = \sum_{s \in S} \sum_{e \in E} \sum_{a \in Attr(e)} w_s \cdot \ell_a(\alpha^{(*)}, \alpha^{(s)}) \\
& \text{s.t } ||W||_2 =1. \nonumber 
\end{align}

where: 

\begin{itemize}
\item $W$ is a $1\times|S|$ vector, $w_s$ is the value associated with source $s$. This value is a measure of the trustworthiness of the source. 
\item $\ell_a$ is a loss function: $D_a \times D_a \rightarrow \mathbb{R}$ where $D_a$ is the space of values that $a$ can have. 
\item $\alpha^{(*)}$ is an estimate of the true value of $a$. 
\end{itemize}

This framework suggests that an assignment of values to attributes of entities should jointly optimize the trustworthiness of the providers and minimize expected difference between the provided value and the true value. The authors prove that this optimization problem is convex and can be solved in closed form for certain loss functions. 

The weight updates are performed by computing the loss between the estimated true values and values provided by the sources. 

\begin{align}
w_s = - \log \left ( \frac{\sum_{e \in E} \sum_{a \in Attr(e)} \ell_a(\alpha^{(*)}, \alpha^{(s)})}{\sum_{s' \in S} \sum_{e \in E}  \sum_{a \in Attr(e)} \ell_a(\alpha^{(*)}, \alpha^{(s')})} \right )
\end{align}

The assignment of each attribute's value for each entity is done by selecting the value which minimizes the loss : 

\begin{align}
\alpha^{(*)} = \argmin_\alpha \sum_{s \in S} w_s \cdot \ell_a(\alpha^{(*)}, \alpha^{(s)})
\end{align}

The algorithm alternates updating $W$ and each $\alpha^{(*)}$ until a convergence condition is met. For each entity $e$, the $\alpha^{(*)}$ from the last iteration of the algorithm is the value assigned to the attribute $a$. This algorithm is summarized in Algorithm \ref{alg:crh}. 


\begin{algorithm}
\small
\caption{Conflict Resolution on Heterogenous Data}
\begin{algorithmic}[1]
\Function{CRH}{$R$, $\delta$} \\
\Comment{Inputs: The collection of records $R$ and the hyper parameter.} \\
\Comment{Output: Returns the fused data $\hat{R}$} \\

\State Initialize $W$ \\

\State Initialize a value $\alpha^{(*)}$ for each attribute $a$ of each entity $e$ using Majority Voting or another method. 

\While{\textbf{not} Converged($W, \delta$)}
\For{\textbf{each}  $e$ \textbf{in} $E$}
\For{\textbf{each} $a$ \textbf{in} $Attr(e)$}
\State $w_s \leftarrow - \log \big ( \frac{\sum_{e \in E} \sum_{a \in Attr(e)} \ell_a(\alpha^{(*)}, \alpha^{(s)})}{\sum_{s' \in S} \sum_{e \in E}  \sum_{a \in Attr(e)} \ell_a(\alpha^{(*)}, \alpha^{(s')})} \big )$
\State $\alpha^{(*)} \leftarrow \argmin_\alpha \sum_{s \in S} w_s \cdot \ell_a(\alpha^{(*)}, \alpha^{(s)})$
\EndFor
\EndFor
\EndWhile

\\

\For{\textbf{each}  $e$ \textbf{in} $E$}
\For{\textbf{each} $a$ \textbf{in} $Attr(e)$}
\State $\hat{\alpha} \leftarrow \argmin_\alpha \sum_{s \in S} w_s \cdot \ell_a(\alpha^{(*)}, \alpha^{(s)})$
\EndFor
\State Add record for $e$ with attributes \& values $(a, \hat{\alpha})$ to $\hat{R}$.
\EndFor
\\

\State \textbf{return} $\hat{R}$
\EndFunction
\end{algorithmic}
\label{alg:crh}
\end{algorithm}  


\subsection{Proposed Methods}

We are in the process of developing extensions to these techniques. Our extensions fall into three main categories: ensemble methods; source-attribute pairwise trustworthiness, and attribute dependence. Based on our literature survey, we do not believe that these topics have been thoroughly investigated thus far. 

\vspace{1 cm}
\subsubsection{Ensemble Methods}

Ensembling existing techniques will consider a group of algorithms $\mathcal{A}_1,\mathcal{A}_2,\dots,\mathcal{A}_N$. Each algorithm $\mathcal{A}_i$ takes the as input an $R_s$ for each source $s \in S$ and outputs a collection of records $\hat{R}_{\mathcal{A}_i}$. 

Our first attempt at an ensemble method is to apply the majority voting algorithm on the outputs $\hat{R}_{\mathcal{A}_1}$, $\hat{R}_{\mathcal{A}_2}$, $\dots$,$\hat{R}_{\mathcal{A}_N}$. In this case, each of the algorithms acts a source. We will follow substitute majority voting with the other algorithms. In this way, we have a nested structure in which we learn a \emph{trustworthiness} of each algorithm. This approach is often referred to as \emph{stacking} in the literature. 

\subsubsection{Source Attribute Trustworthiness}

As pointed out in \cite{li:truth}, a data source may reliably provide values for a particular attribute of an entity, but may unreliably provide values for another attribute. In response to this, we give extensions of existing algorithms which model the trustworthiness of a source--attribute pair.  The {\sc TruthFinder} algorithm can be extended by redefining the $\mathsf{CONF}$ and $\mathsf{TRUST}$ functions in the following way: 


\begin{equation}
\mathsf{CONF(\alpha_i^{(s)})} = \sum_{s' \in S} \mathsf{Provides}(s',\alpha_i^{(s)}) \cdot \ln(1-\mathsf{TRUST}(s',a_i))
\end{equation}


\begin{equation}
\mathsf{TRUST}(s,a_i) = \frac{1}{|E|} \sum_{e \in E} \sigma(\gamma \cdot \mathsf{CONF}(\alpha_i^{(s)}))
\end{equation}

where, with a slightly abuse of notation, $\alpha_i^{(s)}$ refers to the value supplied for attribute $a_i$ for each $e$ in the summation. The  $\mathsf{TRUST}$ is now parameterized at the attribute level; the trustworthiness of an attribute from a source is the average of the confidence values for those given attributes. The rest of the {\sc TruthFinder} algorithm remains unchanged. 


In a similar way, the algorithm presented by Li et al in \cite{li:resolving} can be extended to represent source-attribute trustworthiness. The modified objective function includes a term $w_{s,a}$ for the trustworthiness of the source-attribute pair in place of $w_s$ the source trustworthiness. 

\begin{align}
\hat{R}, W^* = \argmin_{R,W} f(R, W) & = \sum_{s \in S} \sum_{e \in E} \sum_{a \in Attr(e)} w_{s,a} \cdot \ell_a(\alpha^{(*)}, \alpha^{(s)}) \\
& \text{s.t } ||W||_2 =1. \nonumber 
\end{align}

where  $W$ is now a $|S|\times A$ matrix, where $w_{s,a}$ is the value associated with source $s$ and attribute $a$. This value is a measure of the trustworthiness of the source. The weight $w_{s,a}$ and attribute value $\alpha^{(*)}$ updates are now: 


\begin{align}
w_{s,a} = - \log \left ( \frac{\sum_{e \in E} \ell_a(\alpha^{(*)}, \alpha^{(s)})}{\sum_{s' \in S} \sum_{e \in E}  \ell_a(\alpha^{(*)}, \alpha^{(s')})} \right )
\end{align}

\begin{align}
\alpha^{(*)} = \argmin_\alpha \sum_{s \in S} w_{s,a} \cdot \ell_a(\alpha^{(*)}, \alpha^{(s)})
\end{align}


\subsubsection{Attribute Dependence}

A Bayesian network is a directed acyclic graph(DAG) whose nodes are random variables and whose edges correspond to influence of one node on another. A directed edge from node a to b (a is then called the parent of b) models the conditional dependence between a and b ie the random variable associated with a child node follows a probabilistic conditional distribution that takes values conditioned on the parent nodes.

A joint distribution of a network can be written as 

\begin{equation}
P(x) = \prod_{i=1}^n P(x_i|x_{p(i)})
\end{equation}

where $x_{p(i)}$ is the parent node of node $x_i$.

Figure 3 shows a bayesian representation of attribute dependence. The edges between the attributes show how attributes influence each other and the edges from source to attribute captures the influence of source on the attribute values. This model captures the overall trustworthiness of the source in contributing to the attribute values.  We propose a modification of this model shown in Figure 4. Instead of modeling the overall source trustworthiness, we model the trustworthiness of each attribute provided by the source. 

For example:


\begin{align}
P(AAPL Open Price|AAPL Close Price, CNN Money,\nonumber  
\\Yahoo Finance, Market Watch) 
\end{align}

is extended to 

\begin{align}
P(AAPL Open Price|AAPL Close Price, CNN Money \nonumber  
\\Open Price,Yahoo Finance Open Price, \nonumber  
\\Market Watch Open Price) 
\end{align}

Representing the exact distribution $P(x)$ using the full DAG involves a huge number of parameters. This can be approximated by a tree structure $P'(x)$ where the KL-Divergence $D(P,P')$ between the two is minimum.  

\begin{equation}
D(P,P') = \sum_x P(x) log \frac{P(x)}{P'(x)}
\end{equation}

As demonstrated by Chow et al. \cite{chow1968approximating}, the best approximation $P'(x)$ is obtained by calculating the Maximum Weight Spanning Tree over nodes in X, where the weight on an edge($X_i$,$X_j$) is  defined by mutual information measure. 

\begin{equation}
I(X_i,X_j) = \sum_{x_i,x_j} P(x_i,x_j) log\frac{P(x_i,x_j)}{P(x_i) P(x_j)}
\end{equation}

We can then perform inference on the approximated tree representation.

%% This was just a thought I had. We can take it out if you think its not practical.  
Another possible way of handling dependence among attributes is an extension of Li et al's algorithm from \cite{li:resolving}. The objective function now includes a term which measures the mutual information between attributes and a second loss function for comparing the values of two attributes.  

\begin{align}
\hat{R}, W^* = \argmin_{R,W} f(R, W) & = \sum_{s \in S} \sum_{e \in E} \sum_{a \in Attr(e)} \Bigg ( w_{s,a} \cdot \ell_a(\alpha^{(*)}, \alpha^{(s)}) \nonumber \\
&  \cdot \sum_{a' \in Attr(e)} MI(a,a') \ell_{a,a'}(\alpha,\alpha')  \Bigg ) \\
& \text{s.t } ||W||_2 =1. \nonumber 
\end{align}

We are still working on this model, particularly how to define the loss function $\ell_{a,a'}(\alpha,\alpha')$. 



\begin{figure}
\centering
\includegraphics[width=5cm]{bn1.png}
\caption{An example of the attribute dependence model, which models source trustworthiness.}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=9cm]{bn2.png}
\caption{An example of the attribute dependence model which models the trustworthiness of each attribute provided by a source}
\end{figure}

\subsection{Evaluation Methods}
We evaluate the source fusion methods mainly with two measures , precision and recall. These methods have been popular in other fields like information retrieval and machine learning where the effectiveness of an algorithm is measured with respect to the intersection between values output by them and the actual gold values. 
We define them more precisely as shown below.

\begin{equation}
\mathsf {Precision =\frac {\sum_{e \in E}  \sum_{\substack{a_g \in Attr^g(e) \\ a_o \in Attr(e)}} Equal(a_g,a_o)}{\sum_{e \in E}  |(Attr(e))| }}
\end{equation}




 \begin{equation}
\mathsf {Recall =\frac {\sum_{e \in E}  \sum_{\substack{a_g \in Attr^g(e) \\ a_o \in Attr(e)}} Equal(a_g,a_o)} {\sum_{e \in E}  |(Attr^g(e))| }}
\end{equation}

where Equal($a_g$,$a_o$) is an indicator function that is 1 if value of $a_g$ is equal to value of $a_o$  and 0 otherwise.

We also plan to relax the evaluation metric. Instead of assuming that the algorithm will output the exact ground truth value for an attribute, we will allow the value to vary within a small interval. 


\begin{algorithm}[H]
\caption{Example Algorithm}
\begin{algorithmic}[1]
\Function{ExampleAlg}{$G_1$, $G_2$, $f$} \\
\Comment{Inputs: Two graphs $G_1 = (V_1,E_1)$ and $G_2=(V_2,E_2)$ and a function $f: V_1 \rightarrow V_2$.} \\
\Comment{Output: Returns ...} \\


\For{\textbf{each} $v_1$ \textbf{in} $V_1$}
\If{$f(v_1) \not \in V_2$}
\State \textbf{return} false
\EndIf
\EndFor
\\

\State \textbf{return} true
\EndFunction
\end{algorithmic}
\end{algorithm}  


\bibliographystyle{abbrv}
\bibliography{references}  

\balancecolumns
\end{document}
